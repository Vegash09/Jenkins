resources:
  jobs:
    etl_job:
      name: "ETL Job - ${bundle.target}"
      
      tasks:
        - task_key: extract_data
          notebook_task:
            notebook_path: ./notebooks/extract.py
            source: WORKSPACE
          new_cluster:
            spark_version: "13.3.x-scala2.12"
            node_type_id: "Standard_DS3_v2"
            num_workers: 2
            spark_conf:
              "spark.databricks.delta.preview.enabled": "true"
      
      max_concurrent_runs: 1
      
      email_notifications:
        on_failure:
          - team@company.com

    data_quality_job:
      name: "Data Quality Check - ${bundle.target}"
      
      tasks:
        - task_key: validate_data
          notebook_task:
            notebook_path: ./notebooks/data_quality.py
            source: WORKSPACE
          new_cluster:
            spark_version: "13.3.x-scala2.12"
            node_type_id: "Standard_DS3_v2"
            num_workers: 1
      
      schedule:
        quartz_cron_expression: "0 0 8 * * ?"
        timezone_id: "UTC"
        pause_status: UNPAUSED
      
      max_concurrent_runs: 1

    transformation_job:
      name: "Data Transformation - ${bundle.target}"
      
      tasks:
        - task_key: bronze_to_silver
          notebook_task:
            notebook_path: ./notebooks/transform.py
            source: WORKSPACE
          new_cluster:
            spark_version: "13.3.x-scala2.12"
            node_type_id: "Standard_DS3_v2"
            autoscale:
              min_workers: 2
              max_workers: 8
      
      timeout_seconds: 3600
      max_concurrent_runs: 1